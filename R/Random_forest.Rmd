---
title: "Random Forest"
output: html_document
editor_options: 
  chunk_output_type: console
---

# load libraries

```{r}
library(pacman)
p_load(lubridate, tidyverse, dplyr, timetools, data.table, ggplot2, hrbrthemes, tidyr, viridis, randomForest)
op <- options(digits.secs = 3)
```

# create training(dev) and test(val) data set

```{r}
Mviv17_34 <- read.csv("E:/Myotis_vivesi/17_34_rm1/Mviv17_34_acc_behav.csv")
Mviv17_38 <- read.csv("E:/Myotis_vivesi/17_38_rm1/Mviv17_38_acc_behav.csv")
Mviv17_49 <- read.csv("E:/Myotis_vivesi/17_49_rm1/Mviv17_49_acc_behav.csv")
Mviv17_60 <- read.csv("E:/Myotis_vivesi/17_60_rm1/Mviv17_60_acc_behav.csv")
Mviv18_06 <- read.csv("E:/Myotis_vivesi/18_06_rm1/Mviv18_06_acc_behav.csv")


data <- Mviv18_06

names(data)
data <- data[, c(66, 1:65)]

data <- na.omit(data)

table(data$behav)

# or take 70% of each behaviour from each bat?
# run model with and without bat without offset correction

# Development, Validation
sample.ind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
dev_18_06 <- data[which(sample.ind == 1),] # development sample, 70% 
val_18_06 <- data[which(sample.ind == 2),] # validation sample, 30%

Mviv18_06 <- data

table(dev_18_06$behav) / table(data$behav)
table(val_18_06$behav) / table(data$behav)



dev <- rbind(dev_17_34, dev_17_38, dev_17_49, dev_17_60, dev_18_06)
val <- rbind(val_17_34, val_17_38, val_17_49, val_17_60, val_18_06)

# check same distributions
table(dev$behav) / nrow(dev)
table(val$behav) / nrow(val)


table(dev$behav)

#### balance behaviours, min buzz = 3438 -> 3400
buzz <- filter(dev, behav == "buzz")
buzz <- slice_sample(buzz, n = 3400)

comm <- filter(dev, behav == "comm")
comm <- slice_sample(comm, n = 3400)

roost <- filter(dev, behav == "roost")
roost <- slice_sample(roost, n = 3400)

search <- filter(dev, behav == "search")
search <- slice_sample(search, n = 3400)

dev <- rbind(buzz, comm, roost, search)

table(dev$behav)
#### 


dev$behav <- as.factor(dev$behav)


save(dev_17_34, dev_17_38, dev_17_49, dev_17_60, dev_18_06, val_17_34, val_17_38, val_17_49, val_17_60, val_18_06, dev, val, file = "E:/Myotis_vivesi/large_model/all/dev_val_all.Rdata")

```

# train RF

```{r}

# load dev and val 
load("E:/Myotis_vivesi/large_model/all/RF_all.Rdata")

dev <- dev[,-67]

set.seed(222)
R.Forest <- randomForest(formula = behav ~ ., data = dev, ntree = 700, importance = TRUE)
# we want to predict "behav" (response variable) using each of the remaining columns of data (predictor variables)

```

# tune parameters ntree and mtry

# tune ntree

```{r}
# First set mtry to default (sqrt of total number of all predictors) and search for the optimal ntree value
# To find best ntree, build RF with different ntree values (100, 200, 300â€¦.,1,000). We build 10 RF classifiers for each ntree value, record the OOB error rate and select ntree value with minimum OOB error

R.Forest

# number of trees that minimizes OOB
error.rate <- as.data.frame(R.Forest$err.rate)
error.rate$ntree <- row.names(error.rate)
(min.OOB <- error.rate[which.min(error.rate$OOB),])


# OOB rates for different ntree and corresponding best ntree value:
# 100 =   25.86% -> 100 0.2586029
# 200 =   25.19% -> 181 0.2498529
# 300 =   25.1%  -> 259 0.2494853
# 400 =   25.01% -> 387 0.2491176
# 500 =   24.99% -> 481 0.2485294
# 600 =   24.85% -> 581 0.2478676
# 700 =   24.74% -> 680 0.2471324
# 800 =   24.84% -> 735 0.2466912
# 900 =   24.82% -> 735 0.2466912
# 1000 =  24.89% -> 735 0.2466912

# --> ntree = 700

CM.dev <- R.Forest$confusion


plot(R.Forest)

# Check error rates for each behaviour
colnames(error.rate)

# Create a plot before running the lines 650 x 400
plot(error.rate$ntree, error.rate[,1], type = "l", ylim = c(0,0.9), ylab = "Error rate", xlab = "Number of trees")

lines(error.rate[,2], col = 1)   #black -> buzz
lines(error.rate[,3], col = 2)   #red   -> comm 
lines(error.rate[,4], col = 3)   #green -> roost
lines(error.rate[,5], col = 4)   #blue  -> search
lines(error.rate[,1], col = 5)   #turquoise -> OOB

abline(v = 680, col = "darkgreen")

legend("topright", c("buzz","comm","roost","search", "OOB", "min.OOB"), lty = rep(1,8), lwd = rep(1,8), col = c(1:5, "darkgreen"))

```

# tune mtry

```{r}
# apply similar procedure such that random forest is run 10 times. The optimal number of predictors selected for split is selected for which out of bag error rate stabilizes and reach minimum

load("E:/Myotis_vivesi/large_model/all/RF_all.Rdata")

set.seed(222)
R.Forest <- randomForest(formula = behav ~ ., data = dev, ntree = 700, mtry = 8, importance = TRUE)

R.Forest
# OOB rates for different mtry:
# 2   = 25.35%
# 4   = 24.87%
# 8   = 24.74%
# 16  = 24.57%
# 32  = 24.45%

# -> mtry = 8

save(dev, val, CM.dev, R.Forest, file = "E:/Myotis_vivesi/large_model/all/RF_all.Rdata")
```

# Variable Importance

```{r}
# 700 x 500
varImpPlot(R.Forest, sort = TRUE, main = "Variable Importance all", n.var = 20)


# Variables ordered in a list 
GINI.all <- data.frame(importance(R.Forest, type = 2, scale = F))
GINI.all$Variables <- row.names(GINI.all)
GINI.all <- GINI.all[order(GINI.all$MeanDecreaseGini, decreasing = TRUE),]
GINI.all <- GINI.all[1:20,]

# 650 x 400
barplot(GINI.all$MeanDecreaseGini, names.arg = GINI.all$Variables, las = 2, bty = "L", ylab = "Mean Gini Decrease all")


GINI.all <- data.frame(importance(R.Forest, type = 2, scale = F))
GINI.all$Variables <- row.names(GINI.all)
GINI.all <- GINI.all[order(GINI.all$MeanDecreaseGini, decreasing = FALSE),]
GINI.all <- GINI.all[45:65,]

# 400 x 500
dotchart(GINI.all$MeanDecreaseGini, labels = GINI.all$Variables, main = "Mean Gini Decrease all")


var.Accuracy.all <- data.frame(importance(R.Forest, type = 1, scale = T))
var.Accuracy.all$Variables <- row.names(var.Accuracy.all)
var.Accuracy.all <- var.Accuracy.all[order(var.Accuracy.all$MeanDecreaseAccuracy, decreasing = TRUE),]
var.Accuracy.all <- var.Accuracy.all[1:20,]

# 650 x 400
barplot(var.Accuracy.all$MeanDecreaseAccuracy, names.arg = var.Accuracy.all$Variables, las = 2, bty = "L", ylab = "Mean Accuracy Decrease all")


var.Accuracy.all <- data.frame(importance(R.Forest, type = 1, scale = T))
var.Accuracy.all$Variables <- row.names(var.Accuracy.all)
var.Accuracy.all <- var.Accuracy.all[order(var.Accuracy.all$MeanDecreaseAccuracy, decreasing = FALSE),]
var.Accuracy.all <- var.Accuracy.all[45:65,]

# 400 x 500
dotchart(var.Accuracy.all$MeanDecreaseAccuracy, labels =  var.Accuracy.all$Variables, main = "Mean Accuracy Decrease all")

```

# validate how good the model is on the test data (val)

```{r}

dev$pred <- predict(R.Forest, dev)
val$pred <- predict(R.Forest, val)

CM.samp = table(val$pred, val$behav) # same as CM.val??


CM.val = matrix(data = 0, nrow = ncol(CM.samp), ncol = ncol(CM.samp), dimnames = list(colnames(CM.samp), colnames(CM.samp)))

for (j in 1:nrow(CM.samp)){
  CM.val[(rownames(CM.samp[j,,drop = F])),] = CM.samp[j,]
}   


Recall = diag(CM.val) / apply(CM.val, 2, sum)
Precision = diag(CM.val) / apply(CM.val, 1, sum)
Accuracy = sum(diag(CM.val)) / sum(CM.val)

# 650 x 400
names(Recall) = c("buzz", "comm", "roost", "search")
plot(Recall ~ Precision, pch = 16, bty = "L", las = 1, ylim = c(0,1), xlim = c(0.3, 1), cex = 1, main = "all")
text(Precision, Recall, names(Recall), pos = 2, cex = 0.9)


save(dev, val, R.Forest, CM.dev, CM.val, Precision, Recall, Accuracy, file = "E:/Myotis_vivesi/large_model/all/RF_all.Rdata")




# plot CM with corresponding precision, recall, accuracy

load(file = "E:/Myotis_vivesi/large_model/all/RF_all.Rdata")

observations <- table(val$behav)
predictions <- table(val$pred)

Precision <- Precision * 100
Recall <- Recall * 100


CM.val <- rbind(CM.val, observations)
CM.val <- cbind(CM.val, predictions)
CM.val[5,5] <- sum(observations)
CM.val <- rbind(CM.val, Precision)
CM.val <- rbind(CM.val, Recall)
CM.val[6,5] <- "Accuracy"
CM.val[7,5] <- Accuracy * 100

write.csv(CM.val, file = "E:/Myotis_vivesi/large_model/all/CM_val_all.csv")


library(kableExtra)

CM.large <- read.csv(file = "E:/Myotis_vivesi/large_model/all/CM_val_all.csv", sep = ";")

CM.large[(is.na(CM.large))] <- ""

names(CM.large)[1] <- "In/Out"

CM.large[1:5,]

CM.large[1:5,] %>%
  kbl(caption = "Confusion matrix: observations (columns) x predictions (rows)") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  column_spec(1, bold = T) %>%
  row_spec(0, bold = T)




## frequency distribution of most important variables

# 600 x 400
plot(dev$behav, dev$mean.VeDBA, xlim = c(0, 5),  ylab = "mean.VeDBA", xlab = "", col = c(2:5))



hist(dev$mean.VeDBA[which(dev$behav == "comm")], xlim = c(0, 1.5), xlab = "mean.VeDBA", main = "", col = "blue")

hist(dev$mean.VeDBA[which(dev$behav == "search")], xlim = c(0, 1.5), xlab = "mean.VeDBA", main = "", col = "green", add = TRUE)

hist(dev$mean.VeDBA[which(dev$behav == "roost")], xlim = c(0, 1.5), xlab = "mean.VeDBA", main = "", col = "yellow", add = TRUE)

hist(dev$mean.VeDBA[which(dev$behav == "buzz")], xlim = c(0, 1.5), xlab = "mean.VeDBA", main = "", col = "red", add = TRUE)



hist(dev$mean.VeDBA[which(dev$behav == "comm" | dev$behav == "search" | dev$behav == "buzz")], xlim = c(0, 1.5), xlab = "mean.VeDBA", col = "green", main = "inactive vs active")

hist(dev$mean.VeDBA[which(dev$behav == "roost")], xlim = c(0, 1.5), xlab = "mean.VeDBA", col = "yellow", add = TRUE)



##### plot frequency distribution

# all behaviours

mytheme <- theme(
                  plot.title = element_text(face = "bold", family = "Helvetica", size = (15), hjust = 0.5), 
                  legend.title = element_text(face = "bold", family = "Helvetica"), 
                  legend.text = element_text(family = "Helvetica"), 
                  axis.title = element_text(face = "bold", family = "Helvetica", size = (12)),
                  axis.text = element_text(family = "Helvetica", size = (10)),
                  axis.line = element_line(size = 1, colour = "black"),
                  panel.background = element_rect(fill = "white")
                  )


# 650 x 400
ggplot(data = dev, aes(x = mean.VeDBA, group = behav, fill = behav)) +
   geom_density(adjust = 1.5, alpha = .4) +
   xlab("mean.VeDBA") +
   ylab("frequency") +
   ggtitle("all") +
   labs(fill = "behaviour") +
   mytheme

# not roost
not_roost <- dev[-which(dev$behav == "roost"),]
ggplot(data = not_roost, aes(x = mean.VeDBA, group = behav, fill = behav)) +
   geom_density(adjust = 1.5, alpha = .4) +
   xlab("mean.VeDBA") +
   ylab("frequency") +
   ggtitle("not_roost") +
   labs(fill = "behaviour") +
   mytheme

ggplot(data = not_roost, aes(x = range.stX, group = behav, fill = behav)) +
   geom_density(adjust = 1.5, alpha = .4) +
   xlab("range.stX") +
   ylab("frequency") +
   ggtitle("not_roost") +
   labs(fill = "behaviour") +
   mytheme

# only buzz & search
buzz_search <- not_roost[-which(not_roost$behav == "comm"),]
ggplot(data = buzz_search, aes(x = mean.VeDBA, group = behav, fill = behav)) +
   geom_density(adjust = 1.5, alpha = .4) +
   xlab("mean.VeDBA") +
   ylab("frequency") +
   ggtitle("buzz_search") +
   labs(fill = "behaviour") +
   mytheme

ggplot(data = buzz_search, aes(x = max.stY, group = behav, fill = behav)) +
   geom_density(adjust = 1.5, alpha = .4) +
   xlab("max.stY") +
   ylab("frequency") +
   ggtitle("all") +
   labs(fill = "behaviour") +
   mytheme

#####


getwd()
setwd("E:/Myotis_vivesi")

data <- read.csv("E:/Myotis_vivesi/Mviv17_38_behav.csv")

load(file = "E:/Myotis_vivesi/RF_all.Rdata")


library(dunn.test)
# Kruskal for diff. in median, supports histograms
kruskal.test(dev$mean.VeDBA ~ as.factor(dev$behav))
kruskal.active_inactive <- capture.output(print(kruskal.test(dev$mean.VeDBA ~ as.factor(dev$behav))))
writeLines(kruskal.active_inactive, con = "Kruskal.txt")

# Dunn for correction of sev. tests
dunn.test(dev$mean.VeDBA, g = as.factor(dev$behav), method = "bonferroni")
dunn.active_inactive <- capture.output(print(dunn.test(dev$mean.VeDBA, g = as.factor(dev$behav), method = "bonferroni")))
writeLines(dunn.active_inactive, con = "Dunn.txt")


# Plot dunn test
pdf("Significance_plot.pdf")

data_active <- data[which(data$behav == "active"), ]
data_active$behav <- as.factor(data_active$behav)

par(bty = "l", mar = c(5,5,5,2))
boxplot(data_active$mean.VeDBA ~ data_active$behav, ylim = c(0,1.9), ylab = "mean.VeDBA", xlab = "active")
lines(c(2,3), c(1.5,1.5))
lines(c(1,3), c(1.8,1.8))
#locator() #search,then esc
text(x = 2.501641, y = 1.619257, "*", cex = 1.7)
text(x = 1.920028, y = 1.920028, "*", cex = 1.7)
dev.off()

```
