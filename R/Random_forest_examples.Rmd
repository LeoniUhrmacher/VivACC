---
title: "Random_forest"
output: html_document
---

# load libraries
```{r}
library(tidyverse)
library(bcpa)
library(tidyr)
library(data.table)
library(lubridate)
library(janitor)
library(ggplot2)
require(PBSmapping)
library(vroom)
op <- options(digits.secs=3)
```

#RF examples
````{r}
########### Example 1 ###########
require(randomForest)
require(MASS) #Package which contains the Boston housing dataset
attach(Boston)
set.seed(101)

##separate train and test set
#training Sample with 300 observations
train = sample(1:nrow(Boston), 300)
?Boston  #to search on the dataset

##train RF
Boston.rf = randomForest(formula = medv ~ . , data = Boston , subset = train, ntree = 500, mtry = 2)
#use variable ′medv′ as the Response variable, which is the Median Housing Value
#which means we want to predict medv using each of the remaining columns of data
#subset: index vector indicating which rows should be used from data

Boston.rf
## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##           Mean of squared residuals: 12.62686
##                     % Var explained: 84.74


##tune paramters
#check how many trees are required (ntrees)
plot(Boston.rf)
# --> ntrees = 400

#check how many variables should be randomly chosen at each split (mtry)
#e.g. try all possible (13) predictors
#growing 400 trees for 13 times i.e for all 13 predictors
oob.err = double(13)
test.err = double(13)

for(mtry in 1:13) {
  rf = randomForest(form, data = Boston , subset = train, mtry = mtry, ntree = 400) 
  oob.err[mtry] = rf$mse[400] #Error of all trees fitted; mse = mean squared error
  
  pred <- predict(rf, Boston[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry] = with(Boston[-train,], mean((medv - pred)^2)) #Mean Squared Test Error
}

#Plotting both Test Error and Out of Bag Error
matplot(1:mtry , cbind(oob.err,test.err), pch = 19 , col = c("red","blue"), type = "b", ylab = "Mean Squared Error", xlab = "Number of Predictors Considered at each Split")
legend("topright", legend = c("Out of Bag Error","Test Error"), pch = 19, col = c("red","blue"))
# --> mtry = 4
#On the Extreme Right Hand Side of the above plot, we considered all possible 13 predictors at each Split which is only Bagging


############## Example 2 ###############
library(randomForest)
data(iris)
set.seed(71)

##separate train and test set
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]

##train the model
rf_classifier = randomForest(Species ~ ., data = training, ntree = 100, mtry = 2, importance = TRUE)

rf_classifier
##  Call:
##    randomForest(formula = Species ~ ., data = training,ntree=100,mtry=2, importance = TRUE) 
##                 Type of random forest: classification
##                       Number of trees: 100
##  No. of variables tried at each split: 2
##  
##          OOB estimate of  error rate: 5.33%
##  Confusion matrix:
##             setosa versicolor virginica class.error
##  setosa         21          0         0  0.00000000
##  versicolor      0         25         2  0.07407407
##  virginica       0          2        25  0.07407407


##Variable importance
varImpPlot(rf_classifier)


##validate how good the model is on the test data
# Validation set assessment #1: looking at confusion matrix
prediction_for_table <- predict(rf_classifier, validation1[,-5])
table(observed = validation1[,5], predicted = prediction_for_table)

##            predicted
##  observed    setosa versicolor virginica
##  setosa         29          0         0
##  versicolor      0         20         3
##  virginica       0          1        22


# Validation set assessment #2: ROC curves and AUC

# Needs to import ROCR package for ROC curve plotting:
install.packages("ROCR")
library(ROCR)

# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier, validation1[,-5], type = "prob")

# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes 
classes <- levels(validation1$Species)
# For each class
for (i in 1:3)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(validation1[,5] == classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(prediction_for_roc_curve[,i], true_values)
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf, main = "ROC Curve", col = pretty_colours[i]) 
 }
 else
 {
     plot(perf, main = "ROC Curve", col = pretty_colours[i], add = TRUE) 
 }
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(auc.perf@y.values)
}


````

# function to tune RF

```{r}
# use tuneRF() to adjust parameters
RF_tuned <- tuneRF(
            x = dev[, 2:40], #define predictor variables
            y = dev$behav, #define response variable
            ntreeTry = 1000,
            mtryStart = 2, 
            stepFactor = 1.5,
            improve = 0.1,
            trace = FALSE #don't show real-time progress
            )
# ntreeTry: The number of trees to build.
# mtryStart: The starting number of predictor variables to consider at each split.
# stepFactor: The factor to increase by until the out-of-bag estimated error stops improving by a certain amount.
# improve: The amount that the out-of-bag error needs to improve by to keep increasing the step factor

# creates a plot with the number of predictors (mtry) on x and the oob estimated error on y
# use number of predictors with lowest oob estimated error

```